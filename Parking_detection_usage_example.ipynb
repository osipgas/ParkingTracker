{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the Parking Tracker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PKLotDetector import PKLotDetector\n",
    "# PKLotDetector is torch convolution neural network contains 3 convolution and 2 fully-connected layers, it is binary classificator\n",
    "\n",
    "from ParkingTracker import ParkingTracker\n",
    "# Parking Tracker is the class that makes all the process, its convitient usage of BoundingBoxDrawer and PKLotDetector\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate bounding boxes, but just for 1 image because camera is static.\n",
    "# load bounding boxes\n",
    "bounding_boxes_path = 'Usage Example/bounding_boxes.pkl'\n",
    "\n",
    "with open(bounding_boxes_path, 'rb') as f:\n",
    "    bounding_boxes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Neural Network weights\n",
    "weights_path = \"pklot_detector.pth\"\n",
    "pklot_detector = PKLotDetector()\n",
    "pklot_detector.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tt.Resize((100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parking tracker\n",
    "parking_tracker = ParkingTracker(pklot_detector, bounding_boxes, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(numpy_image):\n",
    "    numpy_image = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB) / 255\n",
    "    return torch.from_numpy(numpy_image).permute(2, 0, 1).float()\n",
    "\n",
    "\n",
    "def to_uint8(torch_image):\n",
    "    torch_image *= 255\n",
    "    return torch_image.to(torch.uint8)\n",
    "\n",
    "\n",
    "def to_numpy(torch_image):\n",
    "    torch_image = torch_image.permute(1, 2, 0).numpy()\n",
    "    return cv2.cvtColor(torch_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_source = \"/Users/osiprovin/Desktop/ml:dl/CV/Parking detection/Usage Example/parking_video.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "interval = 0.1\n",
    "interval_frames = int(frame_rate * interval)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_video_path = '/Users/osiprovin/Desktop/ml:dl/CV/Parking detection/Usage Example/output.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')  \n",
    "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osiprovin/miniconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    torch_image = to_torch(frame)\n",
    "\n",
    "\n",
    "    if frame_count % interval_frames == 0:\n",
    "        cropped_images = parking_tracker.crop_image(torch_image)\n",
    "        predicts = parking_tracker.detect(cropped_images)\n",
    "\n",
    "\n",
    "    torch_image = to_uint8(torch_image)\n",
    "    drawed_image = parking_tracker.draw_bounding_boxes(torch_image, predicts)\n",
    "    output_image = to_numpy(drawed_image)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Parking Tracker', output_image)\n",
    "    out.write(output_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
